{
  "Name": "名称",
  "The name of the OpenSearch service": "OpenSearch 服务名称",
  "Description": "描述",
  "The description of the OpenSearch service": "OpenSearch 服务描述",
  "VxNet": "私有网络",
  "Choose a vxnet to join": "选择要加入的私有网络",
  "Express Configuration": "快速配置",
  "Test": "测试环境",
  "Pre-prod": "预生产环境",
  "Prod HA": "高可用生产环境",
  "Test: OS * 1, OS M * 1, Logstash * 1, Dashboard * 1; Pre-prod: OS * 2, OS M * 1, Logstash * 1, Dashboard * 1; Prod HA: OS * 3, OS Master * 3, Logstash * 1, Dashboard * 2": "测试环境: OpenSearch 节点 * 1, OpenSearch 专有主节点 * 1, Logstash 节点 * 1, Dashboard 节点 * 1; <br>预生产环境: OpenSearch 节点 * 2, OpenSearch 专有主节点 * 1, Logstash 节点 * 1, Dashboard 节点 * 1; <br>高可用生产环境: OpenSearch 节点 * 3, OpenSearch 专有主节点 * 3, Logstash 节点 * 1, Dashboard 节点 * 2",
  "CPU": "CPU",
  "CPUs of each node": "每个节点的 CPU 数量",
  "Memory": "内存",
  "Memory of each node": "每个节点的内存数量",
  "Node Count": "节点数量",
  "Number of nodes to create": "创建节点数量",
  "Number of master nodes to create": "要创建的 OpenSearch 专有主节点数量, 单节点仅供测试使用, 生产环境请选择至少三个专有主节点",
  "Instance Class": "主机类型",
  "Volume Class": "存储类型",
  "The volume type for each node, such as high performance, high performance plus, NeonSAN":"磁盘类型, 比如性能型、超高性能型、NeoSAN 等",
  "Volume Size": "存储容量",
  "The volume size for each node": "每个节点的存储容量",
  "OpenSearch Node": "OpenSearch 节点（热）",
  "Data nodes with label node.attr.data: hot": "数据节点, 含 label node.attr.data: hot",
  "OpenSearch Master Node": "OpenSearch 专有主节点",
  "OpenSearch Node 2": "OpenSearch 节点（温）",
  "Data nodes with label node.attr.data: warm": "数据节点, 含 label node.attr.data: warm",
  "OpenSearch Node 3": "OpenSearch 节点（冷）",
  "Data nodes with label node.attr.data: cold": "数据节点, 含 label node.attr.data: cold",
  "kbn_node": "Dashboard 节点",
  "Dashboard Node": "Dashboard 节点",
  "Number of Dashboard Nodes to create": "创建节点数量，除了 Dashboard 服务还绑定 VIP 并提供 Opensearch 的负载均衡和故障转移能力，建议创建至少一个节点并通过 VIP 访问 Opensearch(http://<OS VIP>:9200); 单节点 VIP 有单点失败的风险，如有需要可在此处创建两个节点保证 VIP 的高可用；如不创建 Dashboard 节点将无法使用 OS VIP, 只能通过 Opencsearch 节点的 IP 访问",
  "lst_node": "Logstash 节点",
  "Logstash Node": "Logstash 节点",
  "enable_caddy": "是否启动 caddy.serive",
  "Enable caddy service on all nodes.": "在所有节点上启动 caddy.service.",
  "Enable opensearch nodes' http ssl": "是否在 OpenSearch 节点开启 https",
  "Enable opensearch nodes' http ssl or not. Use system provided certificates or custome certificate, if enabled": "是否在 OpenSearch 节点开启 https. 可使用系统提供的证书或者用户自定义证书.",
  "Enable custom CA certificate": "是否使用自定义 CA 证书",
  "Enable custom CA certificate for opensearch nodes or not.": "是否在 OpenSearch 节点上使用自定义 CA 证书.",
  "Custom CA certificate": "自定义 CA 证书",
  "Custom CA certificate for opensearch nodes": "OpenSearch 节点上使用的自定义 CA 证书.",
  "Custom node certificate": "自定义节点证书",
  "Custom certificate for opensearch nodes": "OpenSearch 节点上使用的自定义节点证书.",
  "Custom node certificate key": "自定义节点证书密钥",
  "Custom certificate key for opensearch nodes": "OpenSearch 节点上使用的自定义节点证书密钥.",
  "enable.node.exporter": "是否启动 node_exporter.service",
  "Prometheus exporter for hardware and OS metrics exposed by *NIX kernels with pluggable metric collectors.": "Prometheus 导出器用于由 *NIX 内核公开的硬件和操作系统指标，带有可插入的指标收集器",
  "Settings for opensearch-exporter-plugin. Whether to export detailed index level metrics or not.": "opensearch-exporter-plugin 配置参数. 是否导出索引级别的指标, 默认值: false.",
  "Settings for opensearch-exporter-plugin. Whether to export cluster settings or not.": "opensearch-exporter-plugin 配置参数. 是否导出集群配置, 默认值: false.",
  "Settings for opensearch-exporter-plugin. Whether to export metrics from local node or all nodes.": "opensearch-exporter-plugin 配置参数. 导出当前节点指标或导出所有节点指标, 默认值: _local.",
  "Enable or disable allocation for specific kinds of shards.": "配置 shards 的存放位置, 默认值: all.",
  "The queue_size allows to control the size of the queue of pending requests that have no threads to execute them.": "写队列大小. 当写请求无法被处理时, 将被放入写队列. 默认值: 10000.",
  "The queue_size allows to control the initial size of the queue of pending requests that have no threads to execute them.": "查询队列大小. 当查询请求无法被处理时, 将被放入查询队列. 默认值: 1000.",
  "In order to enable allowing to delete indices via wildcards or _all, set this config to false.":"删除索引时是否需要提供完整索引名, 而不是通配符或_all的形式. 默认值: true.",
  "The cluster.no_master_block settings controls what operations should be rejected when there is no active master.": "未检测到主节点时, 屏蔽对集群的各类操作. 默认值: write.",
  "If the expected number of nodes is not achieved, the recovery process waits for the configured amount of time before trying to recover regardless. Defaults to 5m if one of the expected_nodes settings is configured.": "如果未达到期望的节点数, recovery 动作将在配置的时间后开始. 默认值 5m",
  "Number of data nodes expected in the cluster. Recovery of local shards begins when the expected number of data nodes join the cluster. Defaults to 0.": "集群期望检测到的数据节点数，如果未达到期望数, 将在规定时间后, 执行 recovery 动作. 默认值: 0.",
  "Recover as long as this many data nodes have joined the cluster.": "集群执行 recovery 动作时, 期望已检测到的数据节点数. 默认值: -1.",
  "Enable or disable cross-origin resource sharing, i.e. whether a browser on another origin can execute requests against Opensearch.": "启用或禁用跨域资源共享，注意：此选项设置不当会有安全风险，详情请参阅官方文档 https://www.elastic.co/guide/en/elasticsearch/reference/7.10/modules-http.html",
  "Which origins to allow.": "允许跨域资源共享的域，注意：此选项设置不当会有安全风险，详情请参阅官方文档 https://www.elastic.co/guide/en/elasticsearch/reference/7.10/modules-http.html",
  "Accepts either a percentage or a byte size value. It defaults to 10%, meaning that 10% of the total heap allocated to a node will be used as the indexing buffer size shared across all shards.": "接受百分比值或字节值, 意味着总堆内存的10%将被用于索引缓存被所有分片共享. 默认值: 10%",
  "The max size of the field data cache.": "field缓存数据所能使用的堆内存的最大值. 默认值: -1b.",
  "Controls the memory size for the filter cache , defaults to 10%. Accepts either a percentage value, like 5%, or an exact value, like 512mb.": "控制filter缓存的内存大小, 接受百分比值, 如5%, 或精确值, 如512mb. 默认值: 10%.",
  "The shard-level request cache module caches the local results on each shard. The cache is managed at the node level, and has a default maximum size of 1% of the heap.": "分片级的请求缓存, 这个缓存由分片所在节点进行管理, 由节点的堆内存提供. 默认值: 1%.",
  "node.attr.data": "node.attr.data(热)",
  "node.attr.data (#2)": "node.attr.data(温)",
  "node.attr.data (#3)": "node.attr.data(冷)",
  "Custom attribute for OpenSearch node, used for hot-warm-cold setup.": "OpenSearch 节点的自定义标签(node.attr.data), 用于热-温-冷架构配置",
  "Security settings for scripting, please refer to https://www.elastic.co/guide/en/elasticsearch/reference/8.6/modules-scripting-security.html": "script 的安全设置, 请参考 https://www.elastic.co/guide/en/elasticsearch/reference/8.6/modules-scripting-security.html",
  "Remote hosts for 'reindex' operation have to be explicitly whitelisted in opensearch.yaml using the reindex.remote.whitelist property. It can be set to a comma delimited list of allowed remote host and port combinations (e.g. otherhost:9200, another:9200, 127.0.10.*:9200, localhost:*)": "执行 remote reindex 时设置的远端主机白名单, 多个主机由半角逗号分隔, 如: otherhost:9200, 192.168.1.*:9200, localhost:*, 详情可参考 https://www.elastic.co/guide/en/elasticsearch/reference/5.6/docs-reindex.html#reindex-from-remote",
  "Settings for opensearch-analysis-ik plugin. Specify the location of extensional dictionary from remote.": "opensearch-analysis-ik 插件配置. 指定远程字典的位置",
  "Settings for opensearch-analysis-ik plugin. Specify the location of extensional stopwords dictionary from remote.": "opensearch-analysis-ik 插件配置. 指定远程分词字典的位置",
  "Specifies the read-only URL repositories that snapshots can be restored from. Refer to https://www.elastic.co/guide/en/elasticsearch/reference/8.6/snapshot-settings.html": "只读 URL 仓库的位置, 用于快照恢复, 参考 https://www.elastic.co/guide/en/elasticsearch/reference/8.6/snapshot-settings.html.",
  "The additional configuration in opensearch.yml. e.g. key: value": "额外的 OpenSearch 配置, 保存在 opensearch.yml 配置文件中. 格式 key: value",
  "One of keystore settings, e.g. key=value.": "一项 keystore 配置, 格式 key=value.",
  "The logger.action.level configuration in log4j2.properties.": "日志配置文件 log4j2.properties 中 logger.action.level 配置项.",
  "The rootLogger.level configuration in log4j2.properties.": "日志配置文件 log4j2.properties 中rootLogger.level 配置项.",
  "The logger.deprecation.level configuration in log4j2.properties.": "日志配置文件 log4j2.properties 中 logger.deprecation.level 配置项.",
  "The logger.index_search_slowlog_rolling.level configuration in log4j2.properties.": "日志配置文件 log4j2.properties 中 logger.index_search_slowlog_rolling.level 配置项.",
  "The logger.index_indexing_slowlog.level configuration in log4j2.properties.": "日志配置文件 log4j2.properties 中 logger.index_indexing_slowlog.level 配置项.",
  "The logger.task_detailslog_rolling.level configuration in log4j2.properties.": "日志配置文件 log4j2.properties 中 logger.task_detailslog_rolling.level 配置项.",
  "Clean logs older than n days.": "log4j2 产生日志的保留天数",
  "Whether to enable heap dump on out of memory error.": "当 OOM 时, jvm 是否自动执行 heap dump",
  "Kernel parameter: net.ipv4.tcp_keepalive_intvl, the interval between subsequent keepalive probes, regardless of what the connection has exchanged in the meantime.": "内核参数: net.ipv4.tcp_keepalive_intvl, keepalive 的探测时间间隔.",
  "Kernel parameter: net.ipv4.tcp_keepalive_probes, the number of unacknowledged probes to send before considering the connection dead and notifying the application layer.": "内核参数: net.ipv4.tcp_keepalive_probes, keepalive 在通知应用层连接断开之前, 允许未应答的(unacknowledged)探活请求数量",
  "Kernel parameter: net.ipv4.tcp_keepalive_time, the interval between the last data packet sent (simple ACKs are not considered data) and the first keepalive probe; after the connection is marked to need keepalive, this counter is not used any further.": "内核参数: net.ipv4.tcp_keepalive_time, 最后一次发送数据包之后, 到发送第一个 keepalive 保活请求之间的时间间隔",

  "Whether start cerebro.service.": "是否启动 cerebro.service",
  "HAProxy balance policy": "HAProxy 后端负载均衡策略",
  "Define the load balancing algorithm to be used in a backend": "HAProxy 后端负载均衡策略, 选项详细说明请查阅官方文档: https://cbonte.github.io/haproxy-dconv/1.8/configuration.html#4-balance",
  "Sets the timeout for HAProxy to connect OpenSearch services, can be ms, s, m, or h, e.g. 50s for 50 seconds": "HAProxy 连接(connect) OpenSearch 服务的超时时间, 单位 ms, s, m, h, 例如: 50s 指 50秒",
  "Sets the timeout for HAProxy to get response from OpenSearch services, can be ms, s, m, or h, e.g. 50s for 50 seconds": "HAProxy 等待 OpenSearch 服务返回响应的超时时间，单位 ms, s, m, h, 例如 50s 表示 50 秒",
  "Sets the maximum per-process number of concurrent connections": "HAProxy 每进程的并发连接数, 详情参考官方文档: https://cbonte.github.io/haproxy-dconv/1.8/configuration.html#4-maxconn",
  "Sets the maximum allowed size of the client request body": "HAProxy 客户端请求体的最大允许值, 单位 k m, 例如 20m 表示 20兆字节",

  "Periodically check if the configuration has changed and reload the pipeline. This can also be triggered manually through the SIGHUP signal": "logstash 是否定期检查并重新加载 pipeline 配置",
  "How often to check if the pipeline configuration has changed (in seconds)": "logstash 定期检查 pipeline 配置的时间间隔频, 单位: 秒",
  "Logstash pipeline config": "logstash 的 pipeline 配置, 可参考'logstash 配置示例'标签页进行设置. 注意: 在此输入框设置 pipeline 配置时, 请删除注释行, 避免配置解析失败.",

  "OpenSearch Node IP": "OpenSearch 节点 IP",
  "The node IP of OpenSearch on which to dump JVM heap": "注意：此操作不可取消并会导致 OpenSearch 服务不可用直至 Dump 完成，可能会有数分钟甚至数十分钟停顿，请谨慎操作！此处填写要生成 Heap Dump 文件的 OpenSearch 节点 IP, 请确保此 IP 是选中的角色里的一个节点，否则操作将被忽略；操作完成后可通过访问 http://IP/dump/ 获取",
  "Operation Timeout": "最大等待时间",
  "The dump timeout in seconds": "dump 操作最大等待时间，以秒为单位；如超过此时间仍未完成，将强制停止此操作（已生成的文件片段将保留）",
  "The node IP of OpenSearch on which to clear dump JVM heap": "此处填写要清除 Heap Dump 文件的 OpenSearch 节点 IP, 请确保此 IP 是选中的角色里的一个节点，否则操作将被忽略；若不填则表示清除所有同角色节点上的 Dump 文件；操作完成后可通过访问 http://IP/dump/ 确认",
  "The node IP of OpenSearch on which to restart": "此处填写要重启的 OpenSearch 节点 IP, 请确保此 IP 是选中的角色里的一个节点，否则操作将被忽略；若不填则表示重启所有同角色节点上的 OpenSearch 服务",
  "The restart timeout in seconds": "所有节点重启完成最大等待时间，以秒为单位；本操作每次只重启一个所选中角色的节点（以节点 IP 升序排列），并等待被重启节点所有分片加载完毕后再重启下一个；比如 600 秒表示若总共重启时间超过 10 分钟将停止操作，请结合日志自行检查重启结果；日志文件可以通过 http://IP/logs 访问",
  
  "restart_rolling": "滚动重启",
  "dump": "生成 Heap Dump",
  "clear_dump": "清除 Heap Dump",
  "logstash demo": "logstash 配置示例",
  "Demo config for logstash pipeline.": "logstash pipeline 的配置示例. 参考配置示例中 output 部分的内容以获取连接 OpenSearch 集群的方法.",
  "notice_when_upgrade": "升级前请手动备份 OpenSearch 节点所有 script.* 配置（共 6 个）以及所有 Logstash 节点 /data 目录以外的数据；升级过程中请避免创建新索引或分片以加快升级速度",
  "esvip": "Opensearch VIP",

  "err_code130": "不允许同时删除主节点和数据节点; 请分别删除主节点和数据节点.",
  "err_code131": "无法转移主节点的管理权限; 请确认拥有足够数量的主节点.",
  "err_code132": "等待转移主节点的管理权限超时; 请确认拥有足够数量的主节点.",
  "err_code133": "无法转移待删数据节点上的数据; 调用 REST API 失败.",
  "err_code134": "无法转移待删数据节点上的数据; 存在状态异常的 shards.",
  "err_code135": "转移待删数据节点上的数据超时; 请确保拥有足够的数据节点, 如果数据节点充足并且需要转移的数据偏多时, 可尝试多次执行删除操作, 也可手动转移数据后再执行删除操作.",
  "err_code136": "无法转移待删数据节点上的数据; 存在 closed 状态的索引, 或者文档数为 0 的索引.",
  "err_code137": "修改 OpenSearch 配置超时. 请再次确认修改是否生效, 或工单联系.",
  "err_code138": "服务未启动. 请工单联系.",
  "err_code140": "端口未监听. 请工单联系.",
  "err_code141": "备份 security_config 失败. 请工单联系.",
  "err_code142": "更新 security_config 失败. 请工单联系."
}
