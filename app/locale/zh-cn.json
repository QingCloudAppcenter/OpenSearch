{
  "Name": "名称",
  "Description": "描述",
  "The name of the ELK service": "OpenSearch 服务名称",
  "The description of the OpenSearch service": "OpenSearch 服务描述",
  "VxNet": "私有网络",
  "Choose a vxnet to join": "选择要加入的私有网络",
  "notice_when_upgrade": "升级前请手动备份 OS 节点所有 script.* 配置（共 6 个）以及所有 Logstash 节点 /data 目录以外的数据；升级过程中请避免创建新索引或分片以加快升级速度",
  "Express Configuration": "快速配置",
  "Test": "测试环境",
  "Pre-prod": "预生产环境",
  "Prod HA": "高可用生产环境",
  "Test: OS * 1, OS M * 1, Logstash * 1, Dashboard * 1; Pre-prod: OS * 2, OS M * 1, Logstash * 1, Dashboard * 1; Prod HA: OS * 3, OS Master * 3, Logstash * 1, Dashboard * 2": "测试环境：OpenSearch 节点 * 1，OpenSearch 专有主节点 * 1，Logstash 节点 * 1，Dashboard 节点 * 1；<br>预生产环境：OpenSearch 节点 * 2，OpenSearch 专有主节点 * 1，Logstash 节点 * 1，Dashboard 节点 * 1；<br>高可用生产环境：OpenSearch 节点 * 3，OpenSearch 专有主节点 * 3，Logstash 节点 * 1，Dashboard 节点 * 2",
  "CPU": "CPU",
  "CPUs of each node": "每个节点的 CPU 数量",
  "Memory": "内存",
  "Memory of each node": "每个节点的内存数量",
  "Instance Class": "主机类型",
  "OpenSearch Node": "OpenSearch 节点（热）",
  "OpenSearch Node 2": "OpenSearch 节点（温）",
  "OpenSearch Node 3": "OpenSearch 节点（冷）",
  "node.attr.data": "node.attr.data（热）",
  "node.attr.data (#2)": "node.attr.data（温）",
  "node.attr.data (#3)": "node.attr.data（冷）",
  "Custom attribute for ES node": "OS 节点自定义标签（node.attr.data），可用作热-温-冷架构配置",
  "ES node group can be used to hold hot-warm-cold data by label node.attr.data": "可以用来支持 热-温-冷（Hot-Warm-Cold）数据节点分离架构，这是在日志或监控领域常见的一种架构，详情可参考官方介绍：https://www.elastic.co/blog/implementing-hot-warm-cold-in-elasticsearch-with-index-lifecycle-management",
  "OpenSearch Master Node": "OpenSearch 专有主节点",
  "restart_rolling": "滚动重启",
  "dump": "生成 Heap Dump",
  "clear_dump": "清除 Heap Dump",
  "lst_node": "Logstash 节点",
  "kbn_node": "Dashboard 节点",
  "esvip": "Opensearch VIP",
  "Dashboard Node": "Dashboard 节点",
  "Number of Dashboard Nodes to create": "要创建的 Dashboard 节点数量，除了 Dashboard 服务还绑定 VIP 并提供 Opensearch 的负载均衡和故障转移能力，建议创建至少一个节点并通过 VIP 访问 Opensearch（http://<OS VIP>:9200）；单节点 VIP 有单点失败的风险，如有需要可在此处创建两个节点保证 VIP 的高可用；如不创建 Dashboard 节点将无法使用 OS VIP，只能通过 Opencsearch 节点的 IP 访问",
  "Logstash Node": "Logstash 节点",
  "Node Count": "节点数量",
  "count": "次",
  "esnode_count": "个",
  "Number of nodes for the cluster to create": "要创建的节点数量",
  "Number of ES nodes to create": "要创建的 OS 节点数量，建议选择不少于分片副本数的个数来保证副本都能正常分配；建议配合专有主节点保证集群服务的高可用",
  "Volume Size": "存储容量",
  "The volume size for each ES node": "每个节点的存储容量，OS 数据节点挂载了一块硬盘容量，请选择 30G 的倍数；如果选择的是 NeonSAN，在此请选择 300G 的倍数",
  "The volume size for each node": "每个节点的存储容量",
  "Volume Class": "存储类型",
  "The volume type for each instance, such as high performance, high performance plus":"磁盘类型，比如性能型与超高性能型",
  "The volume type for each node, such as high performance, high performance plus": "每个节点的数据盘类型，性能型或超高性能型最大支持 2T 容量",
  "The volume type for each node, such as high performance, high performance plus, NeonSAN": "每个节点的数据盘类型，性能型或超高性能型最大支持 2T 容量，有大容量需要的用户可以选企业型 NeonSAN 大容量盘（NeonSAN 只有 北京3区-B、上海1区-A、广东2区-A 等部分区域支持，如上面没列出 NeonSAN 的选项，则说明该区域不支持）",
  "instance class": "实例类型",
  "Elasticsearch Master Node": "OpenSearch 专有主节点",
  "OpenSearch Master Node": "OpenSearch 专有主节点",
  "Number of ES master nodes to create": "要创建的 OS 专有主节点数量，单节点仅供测试使用，生产环境请选择至少三节点；大规模集群（OS 节点数 ≥ 10）建议使用专有主节点来保证集群的高可用；如选择创建专有主节点，上面的 OS 节点将只承担数据功能（Data Node）；如选择不创建专有主节点，上面的 OS 节点将同时承担主节点的任务，可参阅官方文档查看详情：https://www.elastic.co/guide/en/elasticsearch/reference/7.10/modules-node.html",
  "ES Node IP": "OS 节点 IP",
  "The node IP of ES on which to dump JVM heap": "注意：此操作不可取消并会导致 OS 服务不可用直至 Dump 完成，可能会有数分钟甚至数十分钟停顿，请谨慎操作！此处填写要生成 Heap Dump 文件的 OS 节点 IP，请确保此 IP 是选中的角色里的一个节点，否则操作将被忽略；操作完成后可通过访问 http://IP/dump/ 获取",
  "Operation Timeout": "最大等待时间",
  "The dump timeout in seconds": "dump 操作最大等待时间，以秒为单位；如超过此时间仍未完成，将强制停止此操作（已生成的文件片段将保留）",
  "The node IP of ES on which to clear dump JVM heap": "此处填写要清除 Heap Dump 文件的 OS 节点 IP，请确保此 IP 是选中的角色里的一个节点，否则操作将被忽略；若不填则表示清除所有同角色节点上的 Dump 文件；操作完成后可通过访问 http://IP/dump/ 确认",
  "The node IP of ES on which to restart": "此处填写要重启的 OS 节点 IP，请确保此 IP 是选中的角色里的一个节点，否则操作将被忽略；若不填则表示重启所有同角色节点上的 OS 服务",
  "The restart timeout in seconds": "所有节点重启完成最大等待时间，以秒为单位；本操作每次只重启一个所选中角色的节点（以节点 IP 升序排列），并等待被重启节点所有分片加载完毕后再重启下一个；比如 600 秒表示若总共重启时间超过 10 分钟将停止操作，请结合日志自行检查重启结果；日志文件可以通过 http://IP/logs 访问",
  "The discovery.zen.no_master_block settings controls what operations should be rejected when there is no active master.": "控制当节点没有活跃的master节点时哪些操作应该被拒绝",
  "If the expected number of nodes is not achieved, the recovery process waits for the configured amount of time before trying to recover regardless. Defaults to 5m if one of the expected_nodes settings is configured.": "如果未达到期望的节点数,recovery过程将在配置的时间后开始recover操作,默认5分钟",
  "Enable or disable cross-origin resource sharing, i.e. whether a browser on another origin can execute requests against Elasticsearch.": "启用或禁用跨域资源共享，注意：此选项设置不当会有安全风险，详情请参阅官方文档 https://www.elastic.co/guide/en/elasticsearch/reference/7.10/modules-http.html",
  "Which origins to allow.": "允许跨域资源共享的域，注意：此选项设置不当会有安全风险，详情请参阅官方文档 https://www.elastic.co/guide/en/elasticsearch/reference/7.10/modules-http.html",
  "Controls the memory size for the filter cache , defaults to 10%. Accepts either a percentage value, like 5%, or an exact value, like 512mb.": "控制filter缓存的内存大小,默认10%,接受百分比值,如5%,或精确值,如512mb",
  "Accepts either a percentage or a byte size value. It defaults to 10%, meaning that 10% of the total heap allocated to a node will be used as the indexing buffer size shared across all shards.": "接受百分比值或字节值.默认10%,意味着总堆内存的10%将被用于索引缓存被所有分片共享",
  "The shard-level request cache module caches the local results on each shard. The cache is managed at the node level, and has a default maximum size of 1% of the heap.": "分片级的请求缓存对每一个分片做本地缓存.这个缓存在节点级进行管理,默认堆内存的1%",
  "Remote hosts for 'reindex' operation have to be explicitly whitelisted in elasticsearch.yaml using the reindex.remote.whitelist property. It can be set to a comma delimited list of allowed remote host and port combinations (e.g. otherhost:9200, another:9200, 127.0.10.*:9200, localhost:*)": "用来 reindex 操作的远程 OS 节点需要加入白名单，多个 OS 节点可以由半角逗号分隔，如：otherhost:9200, 192.168.1.*:9200, localhost:*，详情可参考官方文件：https://www.elastic.co/guide/en/elasticsearch/reference/5.6/docs-reindex.html#reindex-from-remote",
  "Specify script.allowed_types in elasticsearch.yml": "在此指定允许 OS 执行的脚本类型，比如填入 inline 表示只允许执行 inline 类型的脚本；默认为空，表示允许所有类型；如果希望禁止执行所有类型的脚本，在此填入 none",
  "Specify script.allowed_contexts in elasticsearch.yml": "在此指定允许 OS 执行的脚本上下文比如填入 search, update 表示允许执行 search 和 update 这两种上下文；默认为空，表示允许所有上下文；如果希望禁止执行所有上下文，在此填入 none",
  "the interval between subsequent keepalive probes, regardless of what the connection has exchanged in the meantime": "TCP keepalive 探活的时间间隔（一般保持默认即可）",
  "the number of unacknowledged probes to send before considering the connection dead and notifying the application layer": "在通知应用层连接断开之前，允许尝试发送未应答的（unacknowledged）探活请求的数量（一般保持默认即可）",
  "the interval between the last data packet sent (simple ACKs are not considered data) and the first keepalive probe; after the connection is marked to need keepalive, this counter is not used any further": "最后一次发送数据包之后，到发送第一个 keepalive 保活请求之间的时间间隔（一般保持默认即可）",
  "When you run logstash, you use the -f to specify your config file, and this config is what your config file contained in input section": "当你执行logstash,你用-f指定你的配置文件,你的配置文件的input段的内容将由此设置项决定；由于此处不能换行，请把所有注释行（以 # 开头的行）去掉，以防止合并到一行后引起语法错误",
  "When you run logstash, you use the -f to specify your config file, and this config is what your config file contained in filter section": "当你执行logstash,你用-f指定你的配置文件,你的配置文件的filter段的内容将由此设置项决定；由于此处不能换行，请把所有注释行（以 # 开头的行）去掉，以防止合并到一行后引起语法错误",
  "When you run logstash, you use the -f to specify your config file, and this config is what your config file contained in output section": "当你执行logstash,你用-f指定你的配置文件,你的配置文件的output段的内容将由此设置项决定；由于此处不能换行，请把所有注释行（以 # 开头的行）去掉，以防止合并到一行后引起语法错误",
  "When you run logstash, you use the -f to specify your config file, and this config is what your config file contained in output OpenSearch section": "当你执行logstash,你用-f指定你的配置文件,你的配置文件的output段中的 OpenSearch 段的内容将由此设置项决定；由于此处不能换行，请把所有注释行（以 # 开头的行）去掉，以防止合并到一行后引起语法错误",
  "Specify the location of extensional dictionary from remote.": "指定扩展字典的远程位置",
  "Specify the location of extensional stopwords dictionary from remote.": "指定扩展停止词字典的远程位置",
  "This config is used to adding content into Gemfile for logstash": "本设置项用于为logstash的Gemfile文件增加内容",
  "os_cluster_status": "集群健康状态",
  "number_of_nodes": "节点数",
  "cluster_jvm_heap_used_in_percent": "集群JVM堆内存使用百分比",
  "cluster_jvm_threads_count": "集群JVM线程数",
  "relocating_shards": "正在迁移的分片数",
  "unassigned_shards": "未分配的分片数",
  "number_of_pending_tasks": "等待中的任务数",
  "number_of_in_flight_fetch": "执行中的fetch数",
  "task_max_waiting_in_queue_millis": "任务在队列中的最大等待时间",
  "active_shards_percent_as_number": "活跃分片百分比",
  "cluster_indices_count": "集群索引数",
  "cluster_docs_group": "集群文档监控组",
  "cluster_docs_count": "集群文档数",
  "cluster_docs_deleted_count": "集群已删除文档数",
  "cluster_shards_group": "集群分片监控组",
  "cluster_shards_primaries_count": "集群主分片数",
  "cluster_shards_replication_count": "集群副本分片数",
  "initializing_shards": "初始化中的分片数",
  "In order to enable allowing to delete indices via wildcards or _all, set this config to false.": "为了允许在删除索引时使用通配符或_all, 请设置此配置项为false",
  "The max size of the field data cache.": "field缓存数据所能使用的堆内存的最大值",
  "Specify the location of shared file system repository.": "指定共享文件系统仓库的位置",
  "Specify the location of read-only URL repository.": "指定只读URL仓库的位置",
  "The additional configuration in elasticsearch.yml.": "elasticsearch.yml中的附加配置",
  "Periodically check if the configuration has changed and reload the pipeline. This can also be triggered manually through the SIGHUP signal": "定期检查配置文件并自动加载更改过的 pipeline 配置",
  "How often to check if the pipeline configuration has changed (in seconds)": "定期检查配置文件的频率，以秒为单位",
  "Mapping type": "映射类型（mapping types）",
  "ES Proxy Balance Policy": "OS 代理负载均衡策略",
  "Define the load balancing algorithm to be used in a backend": "选项详细说明请查阅官方文档：https://cbonte.github.io/haproxy-dconv/1.8/configuration.html#4-balance",
  "ES Proxy Connect Timeout": "OS 代理连接超时时间",
  "Sets the timeout for HAProxy to connect ES services, can be ms, s, m, or h, e.g. 50s for 50 seconds": "HAProxy 连接后端 OS 服务的超时时间，时间单位可以是 ms（毫秒）、s（秒）、m（分）或者 h（小时），比如 50s 表示 50 秒",
  "ES Proxy Timeout": "OS 代理超时时间",
  "Sets the timeout for HAProxy to get response from ES services, can be ms, s, m, or h, e.g. 50s for 50 seconds": "HAProxy 等待后端 OS 服务返回响应的超时时间，时间单位可以是 ms（毫秒）、s（秒）、m（分）或者 h（小时），比如 50s 表示 50 秒",
  "ES Proxy Max Connections": "OS 代理最大连接数",
  "Sets the maximum per-process number of concurrent connections": "最大并发连接数，超出的客户端连接请求将排队等待，详情请参阅官方文档：https://cbonte.github.io/haproxy-dconv/1.8/configuration.html#4-maxconn",
  "ES Proxy Max Request": "OS 代理最大请求体",
  "Sets the maximum allowed size of the client request body": "设置 OS 代理的客户端请求体的最大允许值，以 MB 为单位，比如 20m 表示最大允许 20m 字节",
  "Enables you to specify a path to mount Kibana at if you are running behind a proxy. Use the server.rewriteBasePath setting to tell Kibana if it should remove the basePath from requests it receives, and to prevent a deprecation warning at startup. This setting cannot end in a slash (/).": "需要在 Kibana 前面加一层代理时，可以通过此参数设置 Kibana 的路径，不能以斜线（/）结尾，详情可参考官方文档：https://www.elastic.co/guide/en/kibana/master/settings.html",
  "Specifies whether Kibana should rewrite requests that are prefixed with server.basePath or require that they are rewritten by your reverse proxy": "是否重写 bashPath，详情可参考官方文档：https://www.elastic.co/guide/en/kibana/master/settings.html",
  "The logger.action.level configuration in log4j2.properties.": "日志配置文件log4j2.properties中的logger.action.level配置项",
  "The rootLogger.level configuration in log4j2.properties.": "日志配置文件log4j2.properties中的rootLogger.level配置项",
  "The logger.deprecation.level configuration in log4j2.properties.": "日志配置文件log4j2.properties中的logger.deprecation.level配置项",
  "The logger.index_search_slowlog_rolling.level configuration in log4j2.properties.": "日志配置文件log4j2.properties中的logger.index_search_slowlog_rolling.level配置项",
  "The logger.index_indexing_slowlog.level configuration in log4j2.properties.": "日志配置文件log4j2.properties中的logger.index_indexing_slowlog.level配置项",
  "Whether to enable heap dump on out of memory error.": "是否启用自动 heap dump",
  "Whether to enable Cerebro.": "是否启用Cerebro",
  "The path of heap dump.": "Heap dump 文件的存储路径",
  "index_pressure_group": "索引压力",
  "node_indexing_pr_percent_c_p": "索引压力百分比（coordinating, primary）",
  "node_indexing_pr_percent_all": "索引压力百分比（all）",
  "index_pressure_percent_group": "索引压力百分比",
  "Clean logs older than n days, n can be changed here.": "OS 节点日志保留天数",
  "Prometheus exporter for hardware and OS metrics exposed by *NIX kernels with pluggable metric collectors.": "Prometheus 导出器用于由 *NIX 内核公开的硬件和操作系统指标，带有可插入的指标收集器",
  "The queue_size allows to control the initial size of the queue of pending requests that have no threads to execute them.": "queue_size 允许控制没有线程来执行它们的待处理请求队列的初始大小",
  "The queue_size allows to control the size of the queue of pending requests that have no threads to execute them.": "queue_size 允许控制没有线程执行它们的待处理请求队列的大小",
  "Log in OpenSearch with the default username for admin.": "OpenSearch Dashboard 超级管理员（admin）帐号名称。创建后不支持修改。",
  "Log in OpenSearch with the default password for admin.": "OpenSearch Dashboard 超级管理员（admin）帐号密码。创建后不支持修改。",
  "err_code132": "无法获取集群健康状态，可分析非删除节点的日志排查原因；为防止数据丢失，请在集群状态为 green 时进行操作，且每次删除不超过 (N-1)/2 个节点；如果已经连续删除过一些节点并且没有重启服务，则可能需要重启一次服务再删除；如果待删除节点是主节点角色，可能会删除失败，可以等主节点角色转移到其他节点后再试；如需协助请工单联系",
  "err_code133": "检测到删除选中节点可能会导致集群状态异常，请尝试每次只删除一个数据节点，如需协助请工单联系",
  "err_code134": "无法关闭待删除节点的 OS 服务，请稍后重试，如需协助请工单联系",
  "err_code140": "无法通知集群迁移待删除节点上的数据，请确保集群状态为 green 后再尝试删除，如需协助请工单联系",
  "err_code141": "待删除节点上的数据无法迁移到其他节点，请先确保有足够的节点和空间容纳待迁移数据，或者先手动迁移数据，如需协助请工单联系",
  "err_code142": "无法计算待删除节点上的文档数，请确保集群状态为 green 后再尝试删除，如需协助请工单联系",
  "err_code143": "待删除节点上仍有文档数据未迁移到其他节点，可通过 OS API（GET /_cluster/allocation/explain）查看详细原因，如需协助请工单联系",
  "err_code144": "为避免数据丢失和确保集群状态为 green，请至少保留两个 OS 节点",
  "err_code145": "检测到某些索引为关闭状态，为避免数据丢失，请先确保所有索引是打开状态再删除节点",
  "err_code146": "未检测到待删除节点，如需协助请工单联系",
  "err_code148": "将待删除节点除去主节点角色时出错，请稍后重试，如果持续失败请工单联系",
  "err_code149": "获取节点角色时出错，请稍后重试，如果持续失败请工单联系",
  "err_code150": "检测到某些节点与预期不一致，如需协助请工单联系",
  "err_code151": "检测到数据节点的角色与预期不一致，这种情况可能是由于增加了专有主节点以后没有重启原来的热节点导致的，可以滚动重启所有热节点以后重试，如需协助请工单联系",
  "err_code152": "为了保证集群正常工作，请至少保留一个专有主节点",
  "err_code153": "主节点数量超过可处理的最大值，请工单联系",
  "err_code1": "待删除节点上的数据未成功迁移到其他节点,请手动迁移或通过工单与我们联系",
  "Must contain a combination of uppercase and lowercase letters and numbers, special characters can be used (including @#%*_+-=), and the length is between 8-32": "必须包含大小写字母和数字的组合, 可使用特殊字符(包括 @#%*_+-=), 长度在 8-32 之间"
}
